# CSV Chunking Optimizer - Full Stack Application

A comprehensive full-stack application for intelligent CSV data chunking, embedding generation, and vector-based retrieval using React frontend and FastAPI backend.

## 🚀 Quick Start

### Prerequisites
- **Python 3.8+** with pip
- **Node.js 16+** with npm
- **Git** (for cloning)

### Installation & Setup

1. **Clone the repository:**
   ```bash
   git clone <repository-url>
   cd UI_CHUNKING
   ```

2. **Manual Setup (Recommended):**

   **Backend Setup (Terminal 1):**
   ```bash
   # Navigate to backend directory
   cd backend
   
   # Create virtual environment
   python -m venv venv
   
   # Activate virtual environment
   # Windows:
   venv\Scripts\activate
   # Linux/Mac:
   source venv/bin/activate
   
   # Install dependencies
   pip install -r requirements.txt
   
   # Install spaCy English model
   python -m spacy download en_core_web_sm
   
   # Start the server
   python -m uvicorn main:app --host 127.0.0.1 --port 8001 --reload
   ```

   **Frontend Setup (Terminal 2):**
   ```bash
   # Navigate to root directory
   cd ..
   
   # Install dependencies
   npm install
   
   # Start the development server
   npm start
   ```

3. **Alternative: Using Startup Scripts:**
   ```bash
   # Windows
   start_backend.bat
   start_frontend.bat
   
   # Linux/Mac
   chmod +x start_backend.sh start_frontend.sh
   ./start_backend.sh
   ./start_frontend.sh
   ```

4. **Access the Application:**
   - Frontend: http://localhost:3000
   - Backend API: http://127.0.0.1:8001
   - API Docs: http://127.0.0.1:8001/docs

## 🏗️ Architecture

### Frontend (React)
- **Location**: Root directory
- **Port**: 3000
- **Features**:
  - 3 Processing Layers (Fast Mode, Config Mode, Deep Config)
  - Real-time progress tracking
  - Interactive CSV preview
  - Vector search interface
  - Download capabilities

### Backend (FastAPI)
- **Location**: `backend/` directory
- **Port**: 8001
- **Features**:
  - RESTful API endpoints
  - Session management
  - File upload handling
  - Processing pipeline
  - Vector storage (ChromaDB)

## 📁 Project Structure

```
UI_CHUNKING/
├── src/                          # React Frontend
│   ├── components/              # Reusable UI components
│   ├── pages/                   # Layer-specific pages
│   ├── features/                # Feature modules
│   │   ├── fast-mode/          # Layer 1 components
│   │   ├── config-mode/        # Layer 2 components
│   │   └── deep-config/        # Layer 3 components
│   ├── api/                     # API integration
│   └── App.js                   # Main application
├── backend/                      # FastAPI Backend
│   ├── src/                     # Core processing modules
│   │   ├── preprocessing/       # Data preprocessing
│   │   ├── chunking/           # Chunking algorithms
│   │   ├── embedding/          # Embedding generation
│   │   ├── storage/            # Vector storage
│   │   └── retrieval/          # Search functionality
│   ├── main.py                  # FastAPI application
│   └── requirements.txt         # Python dependencies
├── start_backend.bat/.sh        # Backend startup scripts
├── start_frontend.bat/.sh       # Frontend startup scripts
└── README.md                    # This file
```

## 🔧 API Endpoints

### Core Endpoints
- `POST /api/upload` - Upload CSV file
- `POST /api/preprocess` - Run data preprocessing
- `POST /api/chunk` - Apply chunking methods
- `POST /api/embed` - Generate embeddings
- `POST /api/store` - Store vectors in ChromaDB
- `POST /api/search` - Search vectors
- `GET /api/models` - Get available models
- `GET /api/session/{id}` - Get session data

### Processing Layers

#### Layer 1: Fast Mode
- **Purpose**: Quick processing with optimized defaults
- **Pipeline**: Upload → Preprocess → Semantic Chunking → SBERT Embeddings → ChromaDB Storage
- **Features**: Automated pipeline, minimal configuration

#### Layer 2: Config Mode
- **Purpose**: Customizable processing with configuration options
- **Pipeline**: Upload → Configurable Preprocessing → Configurable Chunking → Configurable Embeddings → Storage
- **Features**: Configuration panels, method selection

#### Layer 3: Deep Config
- **Purpose**: Advanced processing with full control
- **Pipeline**: Upload → Step-by-step Preprocessing → Advanced Chunking → Model Selection → Storage → Retrieval Testing
- **Features**: Live preview, download options, detailed configuration

## 🛠️ Development

### Backend Development
```bash
cd backend
pip install -r requirements.txt
python -m spacy download en_core_web_sm
python -m uvicorn main:app --host 127.0.0.1 --port 8001 --reload
```

### Frontend Development
```bash
npm install
npm start
```

### API Testing
Visit http://localhost:8000/docs for interactive API documentation.

## 📊 Features

### Data Processing
- **CSV Upload & Validation**: Automatic header normalization
- **Preprocessing**: Type conversion, null handling, duplicate removal
- **Text Processing**: Stopword removal, lemmatization/stemming

### Chunking Methods
- **Fixed Size**: Row-based chunking with overlap
- **Document Based**: Group by key columns, split by token count
- **Semantic**: AI-powered semantic chunking
- **Recursive**: Character-based recursive splitting

### Embedding Models
- **SBERT Models**: all-MiniLM-L6-v2, all-mpnet-base-v2
- **Batch Processing**: Configurable batch sizes
- **Fallback Mechanisms**: Robust error handling

### Vector Storage
- **ChromaDB Integration**: Efficient vector storage
- **Metadata Filtering**: Numeric and categorical filters
- **Collection Management**: Multiple collections support

### Search & Retrieval
- **Vector Search**: Cosine similarity, dot product, euclidean
- **Metadata Filtering**: Advanced filtering capabilities
- **Metrics Tracking**: Precision, recall, F1-score

## 🔍 Usage Examples

### Basic Processing (Layer 1)
1. Upload CSV file
2. Click "Start Fast Mode Processing"
3. Wait for completion
4. Use search interface to query data

### Advanced Processing (Layer 3)
1. Upload CSV file
2. Configure preprocessing steps
3. Select chunking method and parameters
4. Choose embedding model
5. Configure storage options
6. Test retrieval with queries

## 🐛 Troubleshooting

### Common Issues

1. **Backend not starting**:
   - Check Python version (3.8+)
   - Install dependencies: `pip install -r backend/requirements.txt`
   - Check port 8000 availability

2. **Frontend not connecting**:
   - Ensure backend is running on port 8000
   - Check CORS settings in backend
   - Verify API endpoints in browser dev tools

3. **File upload issues**:
   - Ensure file is valid CSV
   - Check file size limits
   - Verify backend file handling

### Debug Mode
- Backend logs: Check terminal output
- Frontend logs: Browser developer console
- API testing: http://localhost:8000/docs

## 📈 Performance

### Optimization Features
- **Batch Processing**: Configurable batch sizes for embeddings
- **Memory Management**: Efficient data handling
- **Caching**: Session-based data persistence
- **Async Processing**: Non-blocking API calls

### Scalability
- **Session Management**: Multiple concurrent users
- **Vector Storage**: ChromaDB for efficient retrieval
- **API Design**: RESTful architecture for easy scaling

## 🤝 Contributing

1. Fork the repository
2. Create a feature branch
3. Make your changes
4. Test thoroughly
5. Submit a pull request

## 📄 License

This project is licensed under the MIT License.

## 🙏 Acknowledgments

- **Streamlit**: Original web interface inspiration
- **ChromaDB**: Vector storage and retrieval
- **sentence-transformers**: Embedding generation
- **FastAPI**: Modern Python web framework
- **React**: Frontend framework

## 📞 Support

For issues and questions:
1. Check the troubleshooting section
2. Review API documentation at http://localhost:8000/docs
3. Create an issue in the repository
4. Include error messages and system information

---

**Built with ❤️ using React, FastAPI, and modern web technologies**